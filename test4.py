import streamlit as st
from langchain.chains.conversation.base import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
import os
import tempfile
import threading
import json
import librosa
import soundfile as sf
from aip import AipSpeech
import speech_recognition as sr
import uuid  # ç”¨äºç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
import wave
import audioop
import keyboard

# è®¾ç½®ç¼–ç 
os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["LANG"] = "en_US.UTF-8"

# ç™¾åº¦è¯­éŸ³è¯†åˆ«é…ç½®ï¼ˆæ›¿æ¢ä¸ºä½ çš„APIä¿¡æ¯ï¼‰
BAIDU_APP_ID = "119055690"
BAIDU_API_KEY = "Ui908c5FJoScXlNyK7zuXR1Z"
BAIDU_SECRET_KEY = "uqckQD7hKf90VgfjFnaclil4MZ8piEnE"
baidu_client = AipSpeech(BAIDU_APP_ID, BAIDU_API_KEY, BAIDU_SECRET_KEY)

# é€‰æ‹©APIæä¾›å•†
API_PROVIDERS = {
    "twapi": "https://twapi.openai-hk.com/v1",
    "nengyongai": "https://ai.nengyongai.cn",

}


def get_ai_response(user_prompt):
    try:
        # åˆ›å»ºLLMå®ä¾‹
        model = ChatOpenAI(
            model='gpt-4o-mini',
            api_key=st.session_state.get('API_KEY', ''),
            base_url=st.session_state.get('API_BASE_URL', API_PROVIDERS["nengyongai"])
        )

        # åˆ›å»ºå¯¹è¯é“¾
        chain = ConversationChain(llm=model, memory=st.session_state['memory'])

        # è·å–AIå“åº”
        response = chain.invoke({'input': user_prompt})['response']
        return response

    except Exception as e:
        # æ•è·å¹¶å¤„ç†APIè°ƒç”¨å¼‚å¸¸
        error_msg = f"è·å–AIå“åº”å¤±è´¥: {str(e)}"
        st.error(error_msg)
        return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥æˆ–ç¨åå†è¯•ã€‚"


def convert_mp3_to_wav(mp3_path, wav_path='temp.wav'):
    """ä½¿ç”¨çº¯Pythonæ–¹æ¡ˆå°†MP3è½¬æ¢ä¸ºWAVæ ¼å¼"""
    try:
        y, sr = librosa.load(mp3_path, sr=16000)  # åŠ è½½å¹¶é‡é‡‡æ ·åˆ°16kHz
        sf.write(wav_path, y, sr, subtype='PCM_16')  # ä¿å­˜ä¸º16ä½PCM WAV
        return wav_path
    except Exception as e:
        print(f"Librosaè½¬æ¢å¤±è´¥: {str(e)}")
        return None


def speech_to_text(audio_path, is_mp3=False):
    wav_path = None
    try:
        if is_mp3:
            # ä»…ä½¿ç”¨librosaæ–¹æ¡ˆï¼ˆæ— éœ€ffmpegï¼‰
            wav_path = convert_mp3_to_wav(audio_path)
            if not wav_path:
                return "MP3è½¬æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥librosaåº“"
            audio_file = wav_path
        else:
            audio_file = audio_path

        result = baidu_client.asr(
            get_file_content(audio_file),
            'wav', 16000,
            {'dev_pid': 1537}
        )

        if result.get('err_no') == 0:
            return ''.join(result.get('result', []))
        else:
            return f"è¯†åˆ«å¤±è´¥: {result.get('err_msg', 'æœªçŸ¥é”™è¯¯')}"
    except Exception as e:
        return f"å‘ç”Ÿå¼‚å¸¸: {str(e)}"
    finally:
        if wav_path and os.path.exists(wav_path):
            os.remove(wav_path)


def get_file_content(filePath):
    with open(filePath, 'rb') as fp:
        return fp.read()


def handle_uploaded_file(file_input):
    if isinstance(file_input, str):  # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„
        file_path = file_input
        file_name = os.path.basename(file_path)
    else:  # å¦‚æœæ˜¯ UploadedFile å¯¹è±¡
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜ä¸Šä¼ çš„å†…å®¹
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_input.name)[1]) as f:
            f.write(file_input.getbuffer())
            file_path = f.name
            file_name = file_input.name

    is_mp3 = file_name.lower().endswith('.mp3')
    text = speech_to_text(file_path, is_mp3)

    # å¦‚æœæ˜¯ä¸´æ—¶æ–‡ä»¶ï¼Œå¤„ç†å®Œååˆ é™¤
    if not isinstance(file_input, str):
        os.remove(file_path)

    return text


# åˆå§‹åŒ–å½•éŸ³çŠ¶æ€
def init_recording_state():
    if 'is_recording' not in st.session_state:
        st.session_state.is_recording = False
    if 'audio_frames' not in st.session_state:
        st.session_state.audio_frames = []
    if 'recording_thread' not in st.session_state:
        st.session_state.recording_thread = None
    if 'r' not in st.session_state:
        st.session_state.r = sr.Recognizer()

# é¡µé¢è®¾ç½®
st.set_page_config(
    page_title="æˆ‘çš„ChatGPT",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title('æˆ‘çš„ChatGPT')

with st.sidebar:
    st.header("é…ç½®")

    # APIæä¾›å•†é€‰æ‹©
    api_provider = st.selectbox(
        "é€‰æ‹©APIæä¾›å•†",
        list(API_PROVIDERS.keys()),
        index=0,
        key="api_provider_select"
    )

    st.session_state['API_BASE_URL'] = API_PROVIDERS[api_provider]

    # APIå¯†é’¥è¾“å…¥
    api_key_input = st.text_input(
        'è¯·è¾“å…¥ä½ çš„API Keyï¼š',
        type='password',
        key="api_key_input"
    )

    # ä¿å­˜APIå¯†é’¥åˆ°session_state
    if api_key_input:
        st.session_state['API_KEY'] = api_key_input

    st.markdown("---")
    st.info("""
    **æç¤ºï¼š**
    - è¯·ç¡®ä¿è¾“å…¥çš„APIå¯†é’¥æœ‰æ•ˆ
    - å®˜æ–¹OpenAI APIéœ€è¦æœ‰æ•ˆçš„OpenAIè´¦æˆ·
    - ç¬¬ä¸‰æ–¹æœåŠ¡å¯èƒ½æœ‰ä¸åŒçš„ä½¿ç”¨é™åˆ¶
    """)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
init_recording_state()  # ç¡®ä¿å½•éŸ³çŠ¶æ€åœ¨ä¸»ç¨‹åºå¼€å§‹æ—¶åˆå§‹åŒ–

if 'messages' not in st.session_state:
    st.session_state['messages'] = [{'role': 'ai', 'content': 'ä½ å¥½ä¸»äººï¼Œæˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ï¼Œæˆ‘å«å°ç¾'}]
    st.session_state['memory'] = ConversationBufferMemory(return_message=True)

if 'voice_text' not in st.session_state:
    st.session_state['voice_text'] = ""

if 'user_input' not in st.session_state:
    st.session_state['user_input'] = ""

if 'submit_voice' not in st.session_state:
    st.session_state['submit_voice'] = False


# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state['messages']:
    role, content = message['role'], message['content']
    st.chat_message(role).write(content)

# æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
uploaded_file = st.file_uploader("ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", type=['mp3', 'wav'])

# æ¸…é™¤æ—§çš„éŸ³é¢‘æ–‡ä»¶å¼•ç”¨
def clear_old_audio_file():
    if 'last_uploaded_file' in st.session_state:
        if os.path.exists(st.session_state['last_uploaded_file']):
            os.remove(st.session_state['last_uploaded_file'])
        st.session_state.pop('last_uploaded_file', None)


# å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
if uploaded_file is not None:
    # é‡ç½®ç”¨æˆ·è¾“å…¥çŠ¶æ€
    st.session_state['voice_text'] = ""
    st.session_state['user_input'] = ""
    st.session_state['submit_voice'] = False
    with st.spinner("æ­£åœ¨å¤„ç†éŸ³é¢‘æ–‡ä»¶..."):
        text = handle_uploaded_file(uploaded_file)
        st.session_state['voice_text'] = text
        st.session_state['user_input'] = text
        st.session_state['submit_voice'] = True
        st.session_state['last_uploaded_file'] = uploaded_file.name  # è®°å½•å½“å‰æ–‡ä»¶å
        st.session_state.pop('uploaded_file', None)  # æ¸…é™¤sessionä¸­çš„æ–‡ä»¶å¼•ç”¨
        uploaded_file = None  # æ¸…é™¤å½“å‰æ–‡ä»¶å¼•ç”¨

class AudioRecorder:
    def __init__(self):
        self.is_recording = False
        self.audio_frames = []
        self.recorder = sr.Recognizer()
        self.output_file = "recording.wav"
        self.thread = None

    def start_recording(self, output_file="recording.wav", mic_index=0):
        self.output_file = output_file
        self.is_recording = True
        self.audio_frames = []

        self.thread = threading.Thread(target=self._record, args=(mic_index,))
        self.thread.daemon = True
        self.thread.start()

        return True

    def _record(self, mic_index):
        with sr.Microphone(device_index=mic_index) as source:
            print("å¼€å§‹å½•éŸ³ï¼ˆç‚¹å‡»ç•Œé¢æŒ‰é’®åœæ­¢ï¼‰...")
            self.recorder.adjust_for_ambient_noise(source, duration=2)

            try:
                while self.is_recording:
                    try:
                        audio = self.recorder.listen(source, timeout=1)
                        self.audio_frames.append(audio.get_raw_data())
                    except sr.WaitTimeoutError:
                        continue
            except Exception as e:
                print(f"å½•éŸ³å¼‚å¸¸: {e}")
                self.is_recording = False

    def stop_recording(self):
        if not self.is_recording:
            return False

        self.is_recording = False
        if self.thread:
            self.thread.join(timeout=2)

        if not self.audio_frames:
            print("\né”™è¯¯: æœªå½•åˆ¶åˆ°éŸ³é¢‘")
            return False

        # éŸ³é¢‘æ ‡å‡†åŒ–å¤„ç†
        try:
            sample_rate = 44100  # é»˜è®¤é‡‡æ ·ç‡ï¼Œå¯æ ¹æ®å®é™…éº¦å…‹é£è°ƒæ•´
            sample_width = 2  # 16ä½

            audio_data = b''.join(self.audio_frames)

            # é‡é‡‡æ ·åˆ°16000Hzï¼ˆå¦‚æœéœ€è¦ï¼‰
            if sample_rate != 16000:
                print(f"å°†éŸ³é¢‘ä» {sample_rate} Hz é‡é‡‡æ ·åˆ° 16000 Hz...")
                resampled_data, _ = audioop.ratecv(
                    audio_data, sample_width, 1, sample_rate, 16000, None
                )
                audio_data = resampled_data
                sample_rate = 16000

            # ä¿å­˜WAVæ–‡ä»¶
            with wave.open(self.output_file, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(sample_width)
                wf.setframerate(sample_rate)
                wf.writeframes(audio_data)

            print(f"\néŸ³é¢‘å·²ä¿å­˜: {os.path.abspath(self.output_file)}")
            return self.output_file

        except Exception as e:
            print(f"\néŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            return False

# å½•åˆ¶éŸ³é¢‘æŒ‰é’®
if 'recorder' not in st.session_state:
    st.session_state.recorder = AudioRecorder()
if not st.session_state.recorder.is_recording:
    if st.button("ğŸ™ï¸"):
        st.session_state.recorder.start_recording()
        st.info("æ­£åœ¨å½•éŸ³...")

else:
    if st.button("ğŸ›‘"):
        output_file = st.session_state.recorder.stop_recording()
        text = handle_uploaded_file(output_file)
        st.session_state['voice_text'] = text
        st.session_state['user_input'] = text
        st.session_state['submit_voice'] = True
        st.session_state['last_uploaded_file'] = output_file  # è®°å½•å½“å‰æ–‡ä»¶å
        os.remove(output_file)  # å¤„ç†å®Œååˆ é™¤å½•éŸ³æ–‡ä»¶
        if output_file:
            st.success(f"å½•éŸ³å®Œæˆ")
            # è¿™é‡Œå¯ä»¥æ·»åŠ éŸ³é¢‘å¤„ç†é€»è¾‘ï¼ˆå¦‚æ’­æ”¾æˆ–ä¸Šä¼ ï¼‰
        else:
            st.warning("å½•éŸ³å¤±è´¥æˆ–æœªå½•åˆ¶åˆ°éŸ³é¢‘")

# è¾“å…¥åŒºåŸŸ
user_input = st.chat_input("è¾“å…¥æ–‡å­—æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
if user_input:
    # æ¸…é™¤éŸ³é¢‘ç›¸å…³çŠ¶æ€
    st.session_state['voice_text'] = ""
    st.session_state['user_input'] = ""
    st.session_state['submit_voice'] = False
    st.session_state.pop('uploaded_file', None)

# å¤„ç†ç”¨æˆ·è¾“å…¥
if user_input or st.session_state.get('submit_voice', False):
    # è·å–APIå¯†é’¥
    api_key = st.session_state.get('API_KEY', '')

    # åŒºåˆ†å¤„ç†éŸ³é¢‘è¾“å…¥å’Œæ–‡æœ¬è¾“å…¥
    if st.session_state.get('submit_voice', False):
        user_input = st.session_state['user_input']
        st.session_state['submit_voice'] = False
        input_type = "éŸ³é¢‘"
    else:
        input_type = "æ–‡æœ¬"

    # æ£€æŸ¥APIå¯†é’¥
    if not api_key:
        st.warning('è¯·åœ¨ä¾§è¾¹æ è¾“å…¥æ‚¨çš„API Keyï¼')
        st.stop()

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.chat_message('human').write(user_input)
    st.session_state['messages'].append({'role': 'human', 'content': user_input})

    # è®°å½•è¾“å…¥ç±»å‹ï¼ˆå¯é€‰ï¼‰
    st.session_state['last_input_type'] = input_type

    # è·å–AIå“åº”
    with st.spinner('AIæ­£åœ¨æ€è€ƒï¼Œè¯·ç­‰å¾…â€¦â€¦'):
        resp_from_ai = get_ai_response(user_input)
        st.chat_message('ai').write(resp_from_ai)
        st.session_state['messages'].append({'role': 'ai', 'content': resp_from_ai})

    # å¤„ç†å®Œåé‡ç½®éŸ³é¢‘è¾“å…¥çŠ¶æ€
    if input_type == "éŸ³é¢‘":
        st.session_state['user_input'] = ""
        st.session_state['voice_text'] = ""